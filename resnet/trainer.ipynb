{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Se1Oh3KJHI7m","executionInfo":{"status":"ok","timestamp":1689849992325,"user_tz":-540,"elapsed":20658,"user":{"displayName":"최석근","userId":"12650075805346531848"}},"outputId":"1b9310b6-f54d-44fa-d1b2-4a04bc5751fd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iAwOohhOEshf","executionInfo":{"status":"ok","timestamp":1689849999766,"user_tz":-540,"elapsed":7443,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","\n","import sys\n","sys.path.append('drive/MyDrive/mlcs/resnet/')\n","import resnet"]},{"cell_type":"code","source":["# hyperparameters\n","BATCH_SIZE = 256\n","LR = 0.001\n","EPOCHS = 40\n","LR_DECAY = 0.1\n","PROGRESS_INTERVAL = 5\n","PATIENCE = 10"],"metadata":{"id":"TceyBibCEzp0","executionInfo":{"status":"ok","timestamp":1689860010621,"user_tz":-540,"elapsed":2,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"STL7mdgIEzr9","executionInfo":{"status":"ok","timestamp":1689857271091,"user_tz":-540,"elapsed":1,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    #transforms.RandomRotation(15), # add\n","    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # add\n","    transforms.ToTensor(),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform_train)\n","test_dataset = torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=transform_test)\n","\n","train_size = int(0.9*len(train_dataset))\n","val_size = len(train_dataset)-train_size\n","\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # 45000\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)  # 5000\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  # 10000"],"metadata":{"id":"nVNWBauZEzuS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689857274492,"user_tz":-540,"elapsed":1819,"user":{"displayName":"최석근","userId":"12650075805346531848"}},"outputId":"dcb7ad44-915e-4c70-98a1-38c7a88f9dac"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# Model Setup"],"metadata":{"id":"RwMvfF8Tfcvm"}},{"cell_type":"code","source":["del model"],"metadata":{"id":"kiQk1OOs7hid","executionInfo":{"status":"ok","timestamp":1689857277360,"user_tz":-540,"elapsed":2,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["model = resnet.ResNet(resnet.BottleNeck, [3, 4, 6, 3], 10, skip=True)\n","model.state_dict().keys()\n","if torch.cuda.is_available() is True:\n","  model.to(device)"],"metadata":{"id":"O7dgGLk-EzwY","executionInfo":{"status":"ok","timestamp":1689857315407,"user_tz":-540,"elapsed":477,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["# Load Model"],"metadata":{"id":"PMY0UErofgxU"}},{"cell_type":"code","source":["model_path ='drive/MyDrive/mlcs/resnet/model34_ep60_add_drop+aug.pth'\n","\n","model = torch.load(model_path)\n","\n","if torch.cuda.is_available() is True:\n","  model.to(device)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9gRgOFfei_D","executionInfo":{"status":"ok","timestamp":1689854361636,"user_tz":-540,"elapsed":3,"user":{"displayName":"최석근","userId":"12650075805346531848"}},"outputId":"a4252ff2-1321-4333-e398-e52afa782d3e"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (max1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layers1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","  )\n","  (layers2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","  )\n","  (layers3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (3): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (4): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (5): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","  )\n","  (layers4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU()\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (skip_connection): Sequential()\n","    )\n","  )\n","  (avgpool): AdaptiveMaxPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=0.001)\n","#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=LR_DECAY)"],"metadata":{"id":"EfQxzo3ZfXpE","executionInfo":{"status":"ok","timestamp":1689860031051,"user_tz":-540,"elapsed":458,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["def train(model, epochs, progress_interval, patience):\n","    train_loss_list = []\n","    val_loss_list = []\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","    for i in range(1, epochs+1):\n","        train_loss = 0\n","        total_cnt=0\n","        correct_cnt = 0\n","        model.train()\n","\n","        for batch_i, (data, target) in enumerate(train_loader):  # 156 batches # 175 batches\n","            if torch.cuda.is_available() is True:\n","                data, target = data.to(device), target.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","            if (batch_i+1)%progress_interval==0:\n","              print(f\"Training Epoch:{i}, Batch:{batch_i+1}, Loss:{train_loss/(batch_i+1):.4f}\")\n","\n","        train_loss /= len(train_loader)\n","        train_loss_list.append(train_loss)\n","        print(f\"Training Epoch:{i}, Average Loss:{train_loss:.4f}\")\n","\n","        #scheduler.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = 0\n","            for batch_i, (data, target) in enumerate(val_loader):  # 40 batches\n","                if torch.cuda.is_available() is True:\n","                    data, target = data.to(device), target.to(device)\n","\n","                output = model(data)\n","                loss = criterion(output, target)\n","                val_loss += loss.item()\n","                total_cnt+=target.size(0)\n","                _,pred_label = torch.max(nn.functional.softmax(output,dim=1),1)\n","                correct_cnt+=(pred_label==target).sum().item()\n","\n","            val_loss /= len(val_loader)\n","            val_loss_list.append(val_loss)\n","            acu = correct_cnt/total_cnt\n","            print(f\"Validation Epoch:{i}, Average Loss:{val_loss:.4f}, Accuracy:{acu:.4f}\")\n","\n","        if val_loss<best_val_loss:\n","          best_val_loss = val_loss\n","          epochs_no_improve=0\n","        else:\n","          epochs_no_improve += 1\n","          if epochs_no_improve == patience:\n","            print(\"Early stopped!\")\n","            break\n","\n","    return model, train_loss_list, val_loss_list"],"metadata":{"id":"Kii7Ss3VFAdd","executionInfo":{"status":"ok","timestamp":1689860032645,"user_tz":-540,"elapsed":2,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["model, train_loss_list, val_loss_list = train(model, EPOCHS, PROGRESS_INTERVAL, PATIENCE)"],"metadata":{"id":"vCWvChP-FAkH","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"589504cc-3e83-4381-96f8-b8dd1f326db7","executionInfo":{"status":"error","timestamp":1689861997452,"user_tz":-540,"elapsed":1963242,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Epoch:1, Batch:5, Loss:0.8181\n","Training Epoch:1, Batch:10, Loss:0.7898\n","Training Epoch:1, Batch:15, Loss:0.7762\n","Training Epoch:1, Batch:20, Loss:0.7803\n","Training Epoch:1, Batch:25, Loss:0.7726\n","Training Epoch:1, Batch:30, Loss:0.7645\n","Training Epoch:1, Batch:35, Loss:0.7582\n","Training Epoch:1, Batch:40, Loss:0.7606\n","Training Epoch:1, Batch:45, Loss:0.7580\n","Training Epoch:1, Batch:50, Loss:0.7572\n","Training Epoch:1, Batch:55, Loss:0.7531\n","Training Epoch:1, Batch:60, Loss:0.7512\n","Training Epoch:1, Batch:65, Loss:0.7465\n","Training Epoch:1, Batch:70, Loss:0.7456\n","Training Epoch:1, Batch:75, Loss:0.7405\n","Training Epoch:1, Batch:80, Loss:0.7403\n","Training Epoch:1, Batch:85, Loss:0.7391\n","Training Epoch:1, Batch:90, Loss:0.7375\n","Training Epoch:1, Batch:95, Loss:0.7361\n","Training Epoch:1, Batch:100, Loss:0.7341\n","Training Epoch:1, Batch:105, Loss:0.7332\n","Training Epoch:1, Batch:110, Loss:0.7325\n","Training Epoch:1, Batch:115, Loss:0.7308\n","Training Epoch:1, Batch:120, Loss:0.7287\n","Training Epoch:1, Batch:125, Loss:0.7295\n","Training Epoch:1, Batch:130, Loss:0.7280\n","Training Epoch:1, Batch:135, Loss:0.7261\n","Training Epoch:1, Batch:140, Loss:0.7260\n","Training Epoch:1, Batch:145, Loss:0.7253\n","Training Epoch:1, Batch:150, Loss:0.7240\n","Training Epoch:1, Batch:155, Loss:0.7224\n","Training Epoch:1, Batch:160, Loss:0.7212\n","Training Epoch:1, Batch:165, Loss:0.7202\n","Training Epoch:1, Batch:170, Loss:0.7203\n","Training Epoch:1, Batch:175, Loss:0.7195\n","Training Epoch:1, Average Loss:0.7203\n","Validation Epoch:1, Average Loss:0.8137, Accuracy:0.7314\n","Training Epoch:2, Batch:5, Loss:0.6529\n","Training Epoch:2, Batch:10, Loss:0.6783\n","Training Epoch:2, Batch:15, Loss:0.6752\n","Training Epoch:2, Batch:20, Loss:0.6784\n","Training Epoch:2, Batch:25, Loss:0.6774\n","Training Epoch:2, Batch:30, Loss:0.6777\n","Training Epoch:2, Batch:35, Loss:0.6810\n","Training Epoch:2, Batch:40, Loss:0.6766\n","Training Epoch:2, Batch:45, Loss:0.6795\n","Training Epoch:2, Batch:50, Loss:0.6831\n","Training Epoch:2, Batch:55, Loss:0.6832\n","Training Epoch:2, Batch:60, Loss:0.6823\n","Training Epoch:2, Batch:65, Loss:0.6826\n","Training Epoch:2, Batch:70, Loss:0.6828\n","Training Epoch:2, Batch:75, Loss:0.6803\n","Training Epoch:2, Batch:80, Loss:0.6787\n","Training Epoch:2, Batch:85, Loss:0.6790\n","Training Epoch:2, Batch:90, Loss:0.6807\n","Training Epoch:2, Batch:95, Loss:0.6805\n","Training Epoch:2, Batch:100, Loss:0.6815\n","Training Epoch:2, Batch:105, Loss:0.6804\n","Training Epoch:2, Batch:110, Loss:0.6808\n","Training Epoch:2, Batch:115, Loss:0.6809\n","Training Epoch:2, Batch:120, Loss:0.6814\n","Training Epoch:2, Batch:125, Loss:0.6830\n","Training Epoch:2, Batch:130, Loss:0.6828\n","Training Epoch:2, Batch:135, Loss:0.6833\n","Training Epoch:2, Batch:140, Loss:0.6835\n","Training Epoch:2, Batch:145, Loss:0.6837\n","Training Epoch:2, Batch:150, Loss:0.6837\n","Training Epoch:2, Batch:155, Loss:0.6827\n","Training Epoch:2, Batch:160, Loss:0.6814\n","Training Epoch:2, Batch:165, Loss:0.6833\n","Training Epoch:2, Batch:170, Loss:0.6844\n","Training Epoch:2, Batch:175, Loss:0.6854\n","Training Epoch:2, Average Loss:0.6856\n","Validation Epoch:2, Average Loss:0.7421, Accuracy:0.7448\n","Training Epoch:3, Batch:5, Loss:0.6721\n","Training Epoch:3, Batch:10, Loss:0.6816\n","Training Epoch:3, Batch:15, Loss:0.6867\n","Training Epoch:3, Batch:20, Loss:0.6878\n","Training Epoch:3, Batch:25, Loss:0.6907\n","Training Epoch:3, Batch:30, Loss:0.6848\n","Training Epoch:3, Batch:35, Loss:0.6849\n","Training Epoch:3, Batch:40, Loss:0.6869\n","Training Epoch:3, Batch:45, Loss:0.6876\n","Training Epoch:3, Batch:50, Loss:0.6872\n","Training Epoch:3, Batch:55, Loss:0.6862\n","Training Epoch:3, Batch:60, Loss:0.6857\n","Training Epoch:3, Batch:65, Loss:0.6884\n","Training Epoch:3, Batch:70, Loss:0.6903\n","Training Epoch:3, Batch:75, Loss:0.6864\n","Training Epoch:3, Batch:80, Loss:0.6844\n","Training Epoch:3, Batch:85, Loss:0.6829\n","Training Epoch:3, Batch:90, Loss:0.6842\n","Training Epoch:3, Batch:95, Loss:0.6816\n","Training Epoch:3, Batch:100, Loss:0.6816\n","Training Epoch:3, Batch:105, Loss:0.6818\n","Training Epoch:3, Batch:110, Loss:0.6821\n","Training Epoch:3, Batch:115, Loss:0.6813\n","Training Epoch:3, Batch:120, Loss:0.6811\n","Training Epoch:3, Batch:125, Loss:0.6791\n","Training Epoch:3, Batch:130, Loss:0.6802\n","Training Epoch:3, Batch:135, Loss:0.6792\n","Training Epoch:3, Batch:140, Loss:0.6788\n","Training Epoch:3, Batch:145, Loss:0.6764\n","Training Epoch:3, Batch:150, Loss:0.6763\n","Training Epoch:3, Batch:155, Loss:0.6768\n","Training Epoch:3, Batch:160, Loss:0.6768\n","Training Epoch:3, Batch:165, Loss:0.6768\n","Training Epoch:3, Batch:170, Loss:0.6774\n","Training Epoch:3, Batch:175, Loss:0.6769\n","Training Epoch:3, Average Loss:0.6768\n","Validation Epoch:3, Average Loss:0.7443, Accuracy:0.7438\n","Training Epoch:4, Batch:5, Loss:0.6730\n","Training Epoch:4, Batch:10, Loss:0.6469\n","Training Epoch:4, Batch:15, Loss:0.6689\n","Training Epoch:4, Batch:20, Loss:0.6634\n","Training Epoch:4, Batch:25, Loss:0.6625\n","Training Epoch:4, Batch:30, Loss:0.6629\n","Training Epoch:4, Batch:35, Loss:0.6626\n","Training Epoch:4, Batch:40, Loss:0.6629\n","Training Epoch:4, Batch:45, Loss:0.6647\n","Training Epoch:4, Batch:50, Loss:0.6640\n","Training Epoch:4, Batch:55, Loss:0.6612\n","Training Epoch:4, Batch:60, Loss:0.6628\n","Training Epoch:4, Batch:65, Loss:0.6627\n","Training Epoch:4, Batch:70, Loss:0.6641\n","Training Epoch:4, Batch:75, Loss:0.6621\n","Training Epoch:4, Batch:80, Loss:0.6635\n","Training Epoch:4, Batch:85, Loss:0.6624\n","Training Epoch:4, Batch:90, Loss:0.6639\n","Training Epoch:4, Batch:95, Loss:0.6634\n","Training Epoch:4, Batch:100, Loss:0.6638\n","Training Epoch:4, Batch:105, Loss:0.6625\n","Training Epoch:4, Batch:110, Loss:0.6606\n","Training Epoch:4, Batch:115, Loss:0.6602\n","Training Epoch:4, Batch:120, Loss:0.6616\n","Training Epoch:4, Batch:125, Loss:0.6617\n","Training Epoch:4, Batch:130, Loss:0.6615\n","Training Epoch:4, Batch:135, Loss:0.6624\n","Training Epoch:4, Batch:140, Loss:0.6620\n","Training Epoch:4, Batch:145, Loss:0.6626\n","Training Epoch:4, Batch:150, Loss:0.6633\n","Training Epoch:4, Batch:155, Loss:0.6645\n","Training Epoch:4, Batch:160, Loss:0.6642\n","Training Epoch:4, Batch:165, Loss:0.6644\n","Training Epoch:4, Batch:170, Loss:0.6639\n","Training Epoch:4, Batch:175, Loss:0.6646\n","Training Epoch:4, Average Loss:0.6647\n","Validation Epoch:4, Average Loss:0.7455, Accuracy:0.7390\n","Training Epoch:5, Batch:5, Loss:0.6486\n","Training Epoch:5, Batch:10, Loss:0.6420\n","Training Epoch:5, Batch:15, Loss:0.6582\n","Training Epoch:5, Batch:20, Loss:0.6576\n","Training Epoch:5, Batch:25, Loss:0.6666\n","Training Epoch:5, Batch:30, Loss:0.6690\n","Training Epoch:5, Batch:35, Loss:0.6653\n","Training Epoch:5, Batch:40, Loss:0.6642\n","Training Epoch:5, Batch:45, Loss:0.6602\n","Training Epoch:5, Batch:50, Loss:0.6582\n","Training Epoch:5, Batch:55, Loss:0.6583\n","Training Epoch:5, Batch:60, Loss:0.6557\n","Training Epoch:5, Batch:65, Loss:0.6561\n","Training Epoch:5, Batch:70, Loss:0.6564\n","Training Epoch:5, Batch:75, Loss:0.6520\n","Training Epoch:5, Batch:80, Loss:0.6523\n","Training Epoch:5, Batch:85, Loss:0.6558\n","Training Epoch:5, Batch:90, Loss:0.6543\n","Training Epoch:5, Batch:95, Loss:0.6528\n","Training Epoch:5, Batch:100, Loss:0.6524\n","Training Epoch:5, Batch:105, Loss:0.6524\n","Training Epoch:5, Batch:110, Loss:0.6551\n","Training Epoch:5, Batch:115, Loss:0.6549\n","Training Epoch:5, Batch:120, Loss:0.6556\n","Training Epoch:5, Batch:125, Loss:0.6539\n","Training Epoch:5, Batch:130, Loss:0.6545\n","Training Epoch:5, Batch:135, Loss:0.6534\n","Training Epoch:5, Batch:140, Loss:0.6536\n","Training Epoch:5, Batch:145, Loss:0.6547\n","Training Epoch:5, Batch:150, Loss:0.6533\n","Training Epoch:5, Batch:155, Loss:0.6543\n","Training Epoch:5, Batch:160, Loss:0.6555\n","Training Epoch:5, Batch:165, Loss:0.6564\n","Training Epoch:5, Batch:170, Loss:0.6557\n","Training Epoch:5, Batch:175, Loss:0.6582\n","Training Epoch:5, Average Loss:0.6583\n","Validation Epoch:5, Average Loss:0.7357, Accuracy:0.7402\n","Training Epoch:6, Batch:5, Loss:0.6974\n","Training Epoch:6, Batch:10, Loss:0.6649\n","Training Epoch:6, Batch:15, Loss:0.6583\n","Training Epoch:6, Batch:20, Loss:0.6622\n","Training Epoch:6, Batch:25, Loss:0.6650\n","Training Epoch:6, Batch:30, Loss:0.6607\n","Training Epoch:6, Batch:35, Loss:0.6597\n","Training Epoch:6, Batch:40, Loss:0.6569\n","Training Epoch:6, Batch:45, Loss:0.6604\n","Training Epoch:6, Batch:50, Loss:0.6632\n","Training Epoch:6, Batch:55, Loss:0.6591\n","Training Epoch:6, Batch:60, Loss:0.6574\n","Training Epoch:6, Batch:65, Loss:0.6556\n","Training Epoch:6, Batch:70, Loss:0.6561\n","Training Epoch:6, Batch:75, Loss:0.6558\n","Training Epoch:6, Batch:80, Loss:0.6572\n","Training Epoch:6, Batch:85, Loss:0.6545\n","Training Epoch:6, Batch:90, Loss:0.6540\n","Training Epoch:6, Batch:95, Loss:0.6546\n","Training Epoch:6, Batch:100, Loss:0.6548\n","Training Epoch:6, Batch:105, Loss:0.6553\n","Training Epoch:6, Batch:110, Loss:0.6559\n","Training Epoch:6, Batch:115, Loss:0.6560\n","Training Epoch:6, Batch:120, Loss:0.6541\n","Training Epoch:6, Batch:125, Loss:0.6532\n","Training Epoch:6, Batch:130, Loss:0.6525\n","Training Epoch:6, Batch:135, Loss:0.6527\n","Training Epoch:6, Batch:140, Loss:0.6526\n","Training Epoch:6, Batch:145, Loss:0.6529\n","Training Epoch:6, Batch:150, Loss:0.6529\n","Training Epoch:6, Batch:155, Loss:0.6536\n","Training Epoch:6, Batch:160, Loss:0.6544\n","Training Epoch:6, Batch:165, Loss:0.6547\n","Training Epoch:6, Batch:170, Loss:0.6551\n","Training Epoch:6, Batch:175, Loss:0.6555\n","Training Epoch:6, Average Loss:0.6551\n","Validation Epoch:6, Average Loss:0.7490, Accuracy:0.7408\n","Training Epoch:7, Batch:5, Loss:0.6847\n","Training Epoch:7, Batch:10, Loss:0.6589\n","Training Epoch:7, Batch:15, Loss:0.6414\n","Training Epoch:7, Batch:20, Loss:0.6285\n","Training Epoch:7, Batch:25, Loss:0.6341\n","Training Epoch:7, Batch:30, Loss:0.6340\n","Training Epoch:7, Batch:35, Loss:0.6429\n","Training Epoch:7, Batch:40, Loss:0.6393\n","Training Epoch:7, Batch:45, Loss:0.6416\n","Training Epoch:7, Batch:50, Loss:0.6433\n","Training Epoch:7, Batch:55, Loss:0.6401\n","Training Epoch:7, Batch:60, Loss:0.6407\n","Training Epoch:7, Batch:65, Loss:0.6418\n","Training Epoch:7, Batch:70, Loss:0.6403\n","Training Epoch:7, Batch:75, Loss:0.6400\n","Training Epoch:7, Batch:80, Loss:0.6384\n","Training Epoch:7, Batch:85, Loss:0.6395\n","Training Epoch:7, Batch:90, Loss:0.6418\n","Training Epoch:7, Batch:95, Loss:0.6422\n","Training Epoch:7, Batch:100, Loss:0.6423\n","Training Epoch:7, Batch:105, Loss:0.6429\n","Training Epoch:7, Batch:110, Loss:0.6441\n","Training Epoch:7, Batch:115, Loss:0.6440\n","Training Epoch:7, Batch:120, Loss:0.6443\n","Training Epoch:7, Batch:125, Loss:0.6456\n","Training Epoch:7, Batch:130, Loss:0.6466\n","Training Epoch:7, Batch:135, Loss:0.6449\n","Training Epoch:7, Batch:140, Loss:0.6447\n","Training Epoch:7, Batch:145, Loss:0.6443\n","Training Epoch:7, Batch:150, Loss:0.6443\n","Training Epoch:7, Batch:155, Loss:0.6432\n","Training Epoch:7, Batch:160, Loss:0.6441\n","Training Epoch:7, Batch:165, Loss:0.6444\n","Training Epoch:7, Batch:170, Loss:0.6449\n","Training Epoch:7, Batch:175, Loss:0.6445\n","Training Epoch:7, Average Loss:0.6447\n","Validation Epoch:7, Average Loss:0.7747, Accuracy:0.7410\n","Training Epoch:8, Batch:5, Loss:0.6899\n","Training Epoch:8, Batch:10, Loss:0.6767\n","Training Epoch:8, Batch:15, Loss:0.6662\n","Training Epoch:8, Batch:20, Loss:0.6681\n","Training Epoch:8, Batch:25, Loss:0.6619\n","Training Epoch:8, Batch:30, Loss:0.6571\n","Training Epoch:8, Batch:35, Loss:0.6513\n","Training Epoch:8, Batch:40, Loss:0.6571\n","Training Epoch:8, Batch:45, Loss:0.6507\n","Training Epoch:8, Batch:50, Loss:0.6480\n","Training Epoch:8, Batch:55, Loss:0.6456\n","Training Epoch:8, Batch:60, Loss:0.6454\n","Training Epoch:8, Batch:65, Loss:0.6444\n","Training Epoch:8, Batch:70, Loss:0.6451\n","Training Epoch:8, Batch:75, Loss:0.6431\n","Training Epoch:8, Batch:80, Loss:0.6465\n","Training Epoch:8, Batch:85, Loss:0.6489\n","Training Epoch:8, Batch:90, Loss:0.6476\n","Training Epoch:8, Batch:95, Loss:0.6447\n","Training Epoch:8, Batch:100, Loss:0.6445\n","Training Epoch:8, Batch:105, Loss:0.6437\n","Training Epoch:8, Batch:110, Loss:0.6413\n","Training Epoch:8, Batch:115, Loss:0.6424\n","Training Epoch:8, Batch:120, Loss:0.6422\n","Training Epoch:8, Batch:125, Loss:0.6419\n","Training Epoch:8, Batch:130, Loss:0.6404\n","Training Epoch:8, Batch:135, Loss:0.6394\n","Training Epoch:8, Batch:140, Loss:0.6399\n","Training Epoch:8, Batch:145, Loss:0.6387\n","Training Epoch:8, Batch:150, Loss:0.6365\n","Training Epoch:8, Batch:155, Loss:0.6373\n","Training Epoch:8, Batch:160, Loss:0.6362\n","Training Epoch:8, Batch:165, Loss:0.6370\n","Training Epoch:8, Batch:170, Loss:0.6366\n","Training Epoch:8, Batch:175, Loss:0.6364\n","Training Epoch:8, Average Loss:0.6363\n","Validation Epoch:8, Average Loss:0.7896, Accuracy:0.7442\n","Training Epoch:9, Batch:5, Loss:0.6794\n","Training Epoch:9, Batch:10, Loss:0.6541\n","Training Epoch:9, Batch:15, Loss:0.6613\n","Training Epoch:9, Batch:20, Loss:0.6604\n","Training Epoch:9, Batch:25, Loss:0.6501\n","Training Epoch:9, Batch:30, Loss:0.6526\n","Training Epoch:9, Batch:35, Loss:0.6474\n","Training Epoch:9, Batch:40, Loss:0.6438\n","Training Epoch:9, Batch:45, Loss:0.6442\n","Training Epoch:9, Batch:50, Loss:0.6420\n","Training Epoch:9, Batch:55, Loss:0.6407\n","Training Epoch:9, Batch:60, Loss:0.6394\n","Training Epoch:9, Batch:65, Loss:0.6379\n","Training Epoch:9, Batch:70, Loss:0.6379\n","Training Epoch:9, Batch:75, Loss:0.6345\n","Training Epoch:9, Batch:80, Loss:0.6350\n","Training Epoch:9, Batch:85, Loss:0.6360\n","Training Epoch:9, Batch:90, Loss:0.6374\n","Training Epoch:9, Batch:95, Loss:0.6368\n","Training Epoch:9, Batch:100, Loss:0.6357\n","Training Epoch:9, Batch:105, Loss:0.6356\n","Training Epoch:9, Batch:110, Loss:0.6380\n","Training Epoch:9, Batch:115, Loss:0.6387\n","Training Epoch:9, Batch:120, Loss:0.6375\n","Training Epoch:9, Batch:125, Loss:0.6360\n","Training Epoch:9, Batch:130, Loss:0.6361\n","Training Epoch:9, Batch:135, Loss:0.6359\n","Training Epoch:9, Batch:140, Loss:0.6370\n","Training Epoch:9, Batch:145, Loss:0.6359\n","Training Epoch:9, Batch:150, Loss:0.6352\n","Training Epoch:9, Batch:155, Loss:0.6354\n","Training Epoch:9, Batch:160, Loss:0.6373\n","Training Epoch:9, Batch:165, Loss:0.6373\n","Training Epoch:9, Batch:170, Loss:0.6356\n","Training Epoch:9, Batch:175, Loss:0.6354\n","Training Epoch:9, Average Loss:0.6360\n","Validation Epoch:9, Average Loss:0.8179, Accuracy:0.7388\n","Training Epoch:10, Batch:5, Loss:0.6208\n","Training Epoch:10, Batch:10, Loss:0.6143\n","Training Epoch:10, Batch:15, Loss:0.6090\n","Training Epoch:10, Batch:20, Loss:0.6034\n","Training Epoch:10, Batch:25, Loss:0.5992\n","Training Epoch:10, Batch:30, Loss:0.6008\n","Training Epoch:10, Batch:35, Loss:0.6028\n","Training Epoch:10, Batch:40, Loss:0.6055\n","Training Epoch:10, Batch:45, Loss:0.6095\n","Training Epoch:10, Batch:50, Loss:0.6091\n","Training Epoch:10, Batch:55, Loss:0.6120\n","Training Epoch:10, Batch:60, Loss:0.6129\n","Training Epoch:10, Batch:65, Loss:0.6144\n","Training Epoch:10, Batch:70, Loss:0.6129\n","Training Epoch:10, Batch:75, Loss:0.6151\n","Training Epoch:10, Batch:80, Loss:0.6176\n","Training Epoch:10, Batch:85, Loss:0.6171\n","Training Epoch:10, Batch:90, Loss:0.6195\n","Training Epoch:10, Batch:95, Loss:0.6219\n","Training Epoch:10, Batch:100, Loss:0.6247\n","Training Epoch:10, Batch:105, Loss:0.6260\n","Training Epoch:10, Batch:110, Loss:0.6266\n","Training Epoch:10, Batch:115, Loss:0.6270\n","Training Epoch:10, Batch:120, Loss:0.6266\n","Training Epoch:10, Batch:125, Loss:0.6259\n","Training Epoch:10, Batch:130, Loss:0.6276\n","Training Epoch:10, Batch:135, Loss:0.6292\n","Training Epoch:10, Batch:140, Loss:0.6286\n","Training Epoch:10, Batch:145, Loss:0.6298\n","Training Epoch:10, Batch:150, Loss:0.6310\n","Training Epoch:10, Batch:155, Loss:0.6310\n","Training Epoch:10, Batch:160, Loss:0.6316\n","Training Epoch:10, Batch:165, Loss:0.6331\n","Training Epoch:10, Batch:170, Loss:0.6335\n","Training Epoch:10, Batch:175, Loss:0.6332\n","Training Epoch:10, Average Loss:0.6333\n","Validation Epoch:10, Average Loss:0.7409, Accuracy:0.7382\n","Training Epoch:11, Batch:5, Loss:0.5864\n","Training Epoch:11, Batch:10, Loss:0.6044\n","Training Epoch:11, Batch:15, Loss:0.6030\n","Training Epoch:11, Batch:20, Loss:0.6140\n","Training Epoch:11, Batch:25, Loss:0.6101\n","Training Epoch:11, Batch:30, Loss:0.6153\n","Training Epoch:11, Batch:35, Loss:0.6163\n","Training Epoch:11, Batch:40, Loss:0.6199\n","Training Epoch:11, Batch:45, Loss:0.6214\n","Training Epoch:11, Batch:50, Loss:0.6216\n","Training Epoch:11, Batch:55, Loss:0.6207\n","Training Epoch:11, Batch:60, Loss:0.6162\n","Training Epoch:11, Batch:65, Loss:0.6141\n","Training Epoch:11, Batch:70, Loss:0.6154\n","Training Epoch:11, Batch:75, Loss:0.6154\n","Training Epoch:11, Batch:80, Loss:0.6182\n","Training Epoch:11, Batch:85, Loss:0.6180\n","Training Epoch:11, Batch:90, Loss:0.6178\n","Training Epoch:11, Batch:95, Loss:0.6173\n","Training Epoch:11, Batch:100, Loss:0.6176\n","Training Epoch:11, Batch:105, Loss:0.6180\n","Training Epoch:11, Batch:110, Loss:0.6207\n","Training Epoch:11, Batch:115, Loss:0.6217\n","Training Epoch:11, Batch:120, Loss:0.6213\n","Training Epoch:11, Batch:125, Loss:0.6211\n","Training Epoch:11, Batch:130, Loss:0.6204\n","Training Epoch:11, Batch:135, Loss:0.6202\n","Training Epoch:11, Batch:140, Loss:0.6208\n","Training Epoch:11, Batch:145, Loss:0.6215\n","Training Epoch:11, Batch:150, Loss:0.6221\n","Training Epoch:11, Batch:155, Loss:0.6225\n","Training Epoch:11, Batch:160, Loss:0.6234\n","Training Epoch:11, Batch:165, Loss:0.6249\n","Training Epoch:11, Batch:170, Loss:0.6247\n","Training Epoch:11, Batch:175, Loss:0.6262\n","Training Epoch:11, Average Loss:0.6271\n","Validation Epoch:11, Average Loss:0.7727, Accuracy:0.7500\n","Training Epoch:12, Batch:5, Loss:0.6329\n","Training Epoch:12, Batch:10, Loss:0.6354\n","Training Epoch:12, Batch:15, Loss:0.6387\n","Training Epoch:12, Batch:20, Loss:0.6351\n","Training Epoch:12, Batch:25, Loss:0.6413\n","Training Epoch:12, Batch:30, Loss:0.6351\n","Training Epoch:12, Batch:35, Loss:0.6288\n","Training Epoch:12, Batch:40, Loss:0.6291\n","Training Epoch:12, Batch:45, Loss:0.6227\n","Training Epoch:12, Batch:50, Loss:0.6196\n","Training Epoch:12, Batch:55, Loss:0.6229\n","Training Epoch:12, Batch:60, Loss:0.6244\n","Training Epoch:12, Batch:65, Loss:0.6242\n","Training Epoch:12, Batch:70, Loss:0.6263\n","Training Epoch:12, Batch:75, Loss:0.6243\n","Training Epoch:12, Batch:80, Loss:0.6224\n","Training Epoch:12, Batch:85, Loss:0.6205\n","Training Epoch:12, Batch:90, Loss:0.6207\n","Training Epoch:12, Batch:95, Loss:0.6218\n","Training Epoch:12, Batch:100, Loss:0.6234\n","Training Epoch:12, Batch:105, Loss:0.6223\n","Training Epoch:12, Batch:110, Loss:0.6224\n","Training Epoch:12, Batch:115, Loss:0.6239\n","Training Epoch:12, Batch:120, Loss:0.6250\n","Training Epoch:12, Batch:125, Loss:0.6277\n","Training Epoch:12, Batch:130, Loss:0.6270\n","Training Epoch:12, Batch:135, Loss:0.6271\n","Training Epoch:12, Batch:140, Loss:0.6284\n","Training Epoch:12, Batch:145, Loss:0.6289\n","Training Epoch:12, Batch:150, Loss:0.6283\n","Training Epoch:12, Batch:155, Loss:0.6272\n","Training Epoch:12, Batch:160, Loss:0.6280\n","Training Epoch:12, Batch:165, Loss:0.6282\n","Training Epoch:12, Batch:170, Loss:0.6293\n","Training Epoch:12, Batch:175, Loss:0.6285\n","Training Epoch:12, Average Loss:0.6285\n","Validation Epoch:12, Average Loss:0.7283, Accuracy:0.7472\n","Training Epoch:13, Batch:5, Loss:0.6182\n","Training Epoch:13, Batch:10, Loss:0.6494\n","Training Epoch:13, Batch:15, Loss:0.6419\n","Training Epoch:13, Batch:20, Loss:0.6356\n","Training Epoch:13, Batch:25, Loss:0.6334\n","Training Epoch:13, Batch:30, Loss:0.6286\n","Training Epoch:13, Batch:35, Loss:0.6256\n","Training Epoch:13, Batch:40, Loss:0.6233\n","Training Epoch:13, Batch:45, Loss:0.6215\n","Training Epoch:13, Batch:50, Loss:0.6180\n","Training Epoch:13, Batch:55, Loss:0.6209\n","Training Epoch:13, Batch:60, Loss:0.6196\n","Training Epoch:13, Batch:65, Loss:0.6175\n","Training Epoch:13, Batch:70, Loss:0.6160\n","Training Epoch:13, Batch:75, Loss:0.6159\n","Training Epoch:13, Batch:80, Loss:0.6151\n","Training Epoch:13, Batch:85, Loss:0.6150\n","Training Epoch:13, Batch:90, Loss:0.6140\n","Training Epoch:13, Batch:95, Loss:0.6135\n","Training Epoch:13, Batch:100, Loss:0.6148\n","Training Epoch:13, Batch:105, Loss:0.6151\n","Training Epoch:13, Batch:110, Loss:0.6147\n","Training Epoch:13, Batch:115, Loss:0.6154\n","Training Epoch:13, Batch:120, Loss:0.6161\n","Training Epoch:13, Batch:125, Loss:0.6177\n","Training Epoch:13, Batch:130, Loss:0.6180\n","Training Epoch:13, Batch:135, Loss:0.6170\n","Training Epoch:13, Batch:140, Loss:0.6196\n","Training Epoch:13, Batch:145, Loss:0.6203\n","Training Epoch:13, Batch:150, Loss:0.6198\n","Training Epoch:13, Batch:155, Loss:0.6197\n","Training Epoch:13, Batch:160, Loss:0.6194\n","Training Epoch:13, Batch:165, Loss:0.6205\n","Training Epoch:13, Batch:170, Loss:0.6199\n","Training Epoch:13, Batch:175, Loss:0.6203\n","Training Epoch:13, Average Loss:0.6201\n","Validation Epoch:13, Average Loss:0.7363, Accuracy:0.7510\n","Training Epoch:14, Batch:5, Loss:0.6214\n","Training Epoch:14, Batch:10, Loss:0.6142\n","Training Epoch:14, Batch:15, Loss:0.6214\n","Training Epoch:14, Batch:20, Loss:0.6292\n","Training Epoch:14, Batch:25, Loss:0.6293\n","Training Epoch:14, Batch:30, Loss:0.6271\n","Training Epoch:14, Batch:35, Loss:0.6298\n","Training Epoch:14, Batch:40, Loss:0.6251\n","Training Epoch:14, Batch:45, Loss:0.6249\n","Training Epoch:14, Batch:50, Loss:0.6295\n","Training Epoch:14, Batch:55, Loss:0.6249\n","Training Epoch:14, Batch:60, Loss:0.6270\n","Training Epoch:14, Batch:65, Loss:0.6271\n","Training Epoch:14, Batch:70, Loss:0.6273\n","Training Epoch:14, Batch:75, Loss:0.6260\n","Training Epoch:14, Batch:80, Loss:0.6248\n","Training Epoch:14, Batch:85, Loss:0.6236\n","Training Epoch:14, Batch:90, Loss:0.6245\n","Training Epoch:14, Batch:95, Loss:0.6253\n","Training Epoch:14, Batch:100, Loss:0.6254\n","Training Epoch:14, Batch:105, Loss:0.6269\n","Training Epoch:14, Batch:110, Loss:0.6248\n","Training Epoch:14, Batch:115, Loss:0.6251\n","Training Epoch:14, Batch:120, Loss:0.6252\n","Training Epoch:14, Batch:125, Loss:0.6252\n","Training Epoch:14, Batch:130, Loss:0.6250\n","Training Epoch:14, Batch:135, Loss:0.6252\n","Training Epoch:14, Batch:140, Loss:0.6263\n","Training Epoch:14, Batch:145, Loss:0.6252\n","Training Epoch:14, Batch:150, Loss:0.6240\n","Training Epoch:14, Batch:155, Loss:0.6222\n","Training Epoch:14, Batch:160, Loss:0.6227\n","Training Epoch:14, Batch:165, Loss:0.6211\n","Training Epoch:14, Batch:170, Loss:0.6203\n","Training Epoch:14, Batch:175, Loss:0.6203\n","Training Epoch:14, Average Loss:0.6204\n","Validation Epoch:14, Average Loss:0.7320, Accuracy:0.7502\n","Training Epoch:15, Batch:5, Loss:0.6193\n","Training Epoch:15, Batch:10, Loss:0.6265\n","Training Epoch:15, Batch:15, Loss:0.6248\n","Training Epoch:15, Batch:20, Loss:0.6317\n","Training Epoch:15, Batch:25, Loss:0.6275\n","Training Epoch:15, Batch:30, Loss:0.6352\n","Training Epoch:15, Batch:35, Loss:0.6347\n","Training Epoch:15, Batch:40, Loss:0.6260\n","Training Epoch:15, Batch:45, Loss:0.6201\n","Training Epoch:15, Batch:50, Loss:0.6164\n","Training Epoch:15, Batch:55, Loss:0.6124\n","Training Epoch:15, Batch:60, Loss:0.6175\n","Training Epoch:15, Batch:65, Loss:0.6178\n","Training Epoch:15, Batch:70, Loss:0.6181\n","Training Epoch:15, Batch:75, Loss:0.6179\n","Training Epoch:15, Batch:80, Loss:0.6167\n","Training Epoch:15, Batch:85, Loss:0.6176\n","Training Epoch:15, Batch:90, Loss:0.6154\n","Training Epoch:15, Batch:95, Loss:0.6137\n","Training Epoch:15, Batch:100, Loss:0.6123\n","Training Epoch:15, Batch:105, Loss:0.6141\n","Training Epoch:15, Batch:110, Loss:0.6119\n","Training Epoch:15, Batch:115, Loss:0.6139\n","Training Epoch:15, Batch:120, Loss:0.6145\n","Training Epoch:15, Batch:125, Loss:0.6142\n","Training Epoch:15, Batch:130, Loss:0.6138\n","Training Epoch:15, Batch:135, Loss:0.6153\n","Training Epoch:15, Batch:140, Loss:0.6158\n","Training Epoch:15, Batch:145, Loss:0.6154\n","Training Epoch:15, Batch:150, Loss:0.6155\n","Training Epoch:15, Batch:155, Loss:0.6163\n","Training Epoch:15, Batch:160, Loss:0.6159\n","Training Epoch:15, Batch:165, Loss:0.6162\n","Training Epoch:15, Batch:170, Loss:0.6156\n","Training Epoch:15, Batch:175, Loss:0.6161\n","Training Epoch:15, Average Loss:0.6169\n","Validation Epoch:15, Average Loss:0.7624, Accuracy:0.7524\n","Training Epoch:16, Batch:5, Loss:0.6082\n","Training Epoch:16, Batch:10, Loss:0.6098\n","Training Epoch:16, Batch:15, Loss:0.6040\n","Training Epoch:16, Batch:20, Loss:0.5997\n","Training Epoch:16, Batch:25, Loss:0.6014\n","Training Epoch:16, Batch:30, Loss:0.6004\n","Training Epoch:16, Batch:35, Loss:0.5997\n","Training Epoch:16, Batch:40, Loss:0.6008\n","Training Epoch:16, Batch:45, Loss:0.6002\n","Training Epoch:16, Batch:50, Loss:0.6052\n","Training Epoch:16, Batch:55, Loss:0.6018\n","Training Epoch:16, Batch:60, Loss:0.6014\n","Training Epoch:16, Batch:65, Loss:0.6018\n","Training Epoch:16, Batch:70, Loss:0.6047\n","Training Epoch:16, Batch:75, Loss:0.6053\n","Training Epoch:16, Batch:80, Loss:0.6040\n","Training Epoch:16, Batch:85, Loss:0.6024\n","Training Epoch:16, Batch:90, Loss:0.6030\n","Training Epoch:16, Batch:95, Loss:0.6033\n","Training Epoch:16, Batch:100, Loss:0.6024\n","Training Epoch:16, Batch:105, Loss:0.6024\n","Training Epoch:16, Batch:110, Loss:0.6004\n","Training Epoch:16, Batch:115, Loss:0.6008\n","Training Epoch:16, Batch:120, Loss:0.6015\n","Training Epoch:16, Batch:125, Loss:0.6010\n","Training Epoch:16, Batch:130, Loss:0.5991\n","Training Epoch:16, Batch:135, Loss:0.6012\n","Training Epoch:16, Batch:140, Loss:0.6018\n","Training Epoch:16, Batch:145, Loss:0.6026\n","Training Epoch:16, Batch:150, Loss:0.6045\n","Training Epoch:16, Batch:155, Loss:0.6050\n","Training Epoch:16, Batch:160, Loss:0.6042\n","Training Epoch:16, Batch:165, Loss:0.6051\n","Training Epoch:16, Batch:170, Loss:0.6067\n","Training Epoch:16, Batch:175, Loss:0.6066\n","Training Epoch:16, Average Loss:0.6067\n","Validation Epoch:16, Average Loss:0.8205, Accuracy:0.7488\n","Training Epoch:17, Batch:5, Loss:0.6275\n","Training Epoch:17, Batch:10, Loss:0.6065\n","Training Epoch:17, Batch:15, Loss:0.6095\n","Training Epoch:17, Batch:20, Loss:0.6027\n","Training Epoch:17, Batch:25, Loss:0.6120\n","Training Epoch:17, Batch:30, Loss:0.6049\n","Training Epoch:17, Batch:35, Loss:0.6046\n","Training Epoch:17, Batch:40, Loss:0.6010\n","Training Epoch:17, Batch:45, Loss:0.5982\n","Training Epoch:17, Batch:50, Loss:0.5962\n","Training Epoch:17, Batch:55, Loss:0.5949\n","Training Epoch:17, Batch:60, Loss:0.5959\n","Training Epoch:17, Batch:65, Loss:0.5945\n","Training Epoch:17, Batch:70, Loss:0.5932\n","Training Epoch:17, Batch:75, Loss:0.5960\n","Training Epoch:17, Batch:80, Loss:0.5944\n","Training Epoch:17, Batch:85, Loss:0.5966\n","Training Epoch:17, Batch:90, Loss:0.5987\n","Training Epoch:17, Batch:95, Loss:0.6020\n","Training Epoch:17, Batch:100, Loss:0.6038\n","Training Epoch:17, Batch:105, Loss:0.6047\n","Training Epoch:17, Batch:110, Loss:0.6044\n","Training Epoch:17, Batch:115, Loss:0.6037\n","Training Epoch:17, Batch:120, Loss:0.6029\n","Training Epoch:17, Batch:125, Loss:0.6032\n","Training Epoch:17, Batch:130, Loss:0.6035\n","Training Epoch:17, Batch:135, Loss:0.6037\n","Training Epoch:17, Batch:140, Loss:0.6026\n","Training Epoch:17, Batch:145, Loss:0.6025\n","Training Epoch:17, Batch:150, Loss:0.6018\n","Training Epoch:17, Batch:155, Loss:0.6025\n","Training Epoch:17, Batch:160, Loss:0.6028\n","Training Epoch:17, Batch:165, Loss:0.6030\n","Training Epoch:17, Batch:170, Loss:0.6022\n","Training Epoch:17, Batch:175, Loss:0.6037\n","Training Epoch:17, Average Loss:0.6035\n","Validation Epoch:17, Average Loss:0.8085, Accuracy:0.7498\n","Training Epoch:18, Batch:5, Loss:0.5881\n","Training Epoch:18, Batch:10, Loss:0.5870\n","Training Epoch:18, Batch:15, Loss:0.5894\n","Training Epoch:18, Batch:20, Loss:0.5949\n","Training Epoch:18, Batch:25, Loss:0.5958\n","Training Epoch:18, Batch:30, Loss:0.5964\n","Training Epoch:18, Batch:35, Loss:0.5943\n","Training Epoch:18, Batch:40, Loss:0.5978\n","Training Epoch:18, Batch:45, Loss:0.5987\n","Training Epoch:18, Batch:50, Loss:0.5956\n","Training Epoch:18, Batch:55, Loss:0.5956\n","Training Epoch:18, Batch:60, Loss:0.5957\n","Training Epoch:18, Batch:65, Loss:0.5993\n","Training Epoch:18, Batch:70, Loss:0.5958\n","Training Epoch:18, Batch:75, Loss:0.5970\n","Training Epoch:18, Batch:80, Loss:0.5965\n","Training Epoch:18, Batch:85, Loss:0.5973\n","Training Epoch:18, Batch:90, Loss:0.5985\n","Training Epoch:18, Batch:95, Loss:0.5986\n","Training Epoch:18, Batch:100, Loss:0.6010\n","Training Epoch:18, Batch:105, Loss:0.6013\n","Training Epoch:18, Batch:110, Loss:0.6009\n","Training Epoch:18, Batch:115, Loss:0.6006\n","Training Epoch:18, Batch:120, Loss:0.5997\n","Training Epoch:18, Batch:125, Loss:0.6010\n","Training Epoch:18, Batch:130, Loss:0.6014\n","Training Epoch:18, Batch:135, Loss:0.6001\n","Training Epoch:18, Batch:140, Loss:0.6028\n","Training Epoch:18, Batch:145, Loss:0.6030\n","Training Epoch:18, Batch:150, Loss:0.6021\n","Training Epoch:18, Batch:155, Loss:0.6035\n","Training Epoch:18, Batch:160, Loss:0.6031\n","Training Epoch:18, Batch:165, Loss:0.6031\n","Training Epoch:18, Batch:170, Loss:0.6022\n","Training Epoch:18, Batch:175, Loss:0.6015\n","Training Epoch:18, Average Loss:0.6013\n","Validation Epoch:18, Average Loss:0.7766, Accuracy:0.7580\n","Training Epoch:19, Batch:5, Loss:0.5851\n","Training Epoch:19, Batch:10, Loss:0.5732\n","Training Epoch:19, Batch:15, Loss:0.5869\n","Training Epoch:19, Batch:20, Loss:0.5904\n","Training Epoch:19, Batch:25, Loss:0.5837\n","Training Epoch:19, Batch:30, Loss:0.5848\n","Training Epoch:19, Batch:35, Loss:0.5826\n","Training Epoch:19, Batch:40, Loss:0.5844\n","Training Epoch:19, Batch:45, Loss:0.5832\n","Training Epoch:19, Batch:50, Loss:0.5819\n","Training Epoch:19, Batch:55, Loss:0.5857\n","Training Epoch:19, Batch:60, Loss:0.5884\n","Training Epoch:19, Batch:65, Loss:0.5898\n","Training Epoch:19, Batch:70, Loss:0.5926\n","Training Epoch:19, Batch:75, Loss:0.5918\n","Training Epoch:19, Batch:80, Loss:0.5934\n","Training Epoch:19, Batch:85, Loss:0.5950\n","Training Epoch:19, Batch:90, Loss:0.5930\n","Training Epoch:19, Batch:95, Loss:0.5923\n","Training Epoch:19, Batch:100, Loss:0.5932\n","Training Epoch:19, Batch:105, Loss:0.5943\n","Training Epoch:19, Batch:110, Loss:0.5928\n","Training Epoch:19, Batch:115, Loss:0.5928\n","Training Epoch:19, Batch:120, Loss:0.5930\n","Training Epoch:19, Batch:125, Loss:0.5944\n","Training Epoch:19, Batch:130, Loss:0.5936\n","Training Epoch:19, Batch:135, Loss:0.5938\n","Training Epoch:19, Batch:140, Loss:0.5934\n","Training Epoch:19, Batch:145, Loss:0.5952\n","Training Epoch:19, Batch:150, Loss:0.5939\n","Training Epoch:19, Batch:155, Loss:0.5950\n","Training Epoch:19, Batch:160, Loss:0.5968\n","Training Epoch:19, Batch:165, Loss:0.5966\n","Training Epoch:19, Batch:170, Loss:0.5968\n","Training Epoch:19, Batch:175, Loss:0.5965\n","Training Epoch:19, Average Loss:0.5967\n","Validation Epoch:19, Average Loss:0.9117, Accuracy:0.7472\n","Training Epoch:20, Batch:5, Loss:0.5876\n","Training Epoch:20, Batch:10, Loss:0.6058\n","Training Epoch:20, Batch:15, Loss:0.6164\n","Training Epoch:20, Batch:20, Loss:0.6164\n","Training Epoch:20, Batch:25, Loss:0.6073\n","Training Epoch:20, Batch:30, Loss:0.5973\n","Training Epoch:20, Batch:35, Loss:0.5942\n","Training Epoch:20, Batch:40, Loss:0.5964\n","Training Epoch:20, Batch:45, Loss:0.5986\n","Training Epoch:20, Batch:50, Loss:0.6011\n","Training Epoch:20, Batch:55, Loss:0.6001\n","Training Epoch:20, Batch:60, Loss:0.5967\n","Training Epoch:20, Batch:65, Loss:0.5943\n","Training Epoch:20, Batch:70, Loss:0.5947\n","Training Epoch:20, Batch:75, Loss:0.5954\n","Training Epoch:20, Batch:80, Loss:0.5930\n","Training Epoch:20, Batch:85, Loss:0.5964\n","Training Epoch:20, Batch:90, Loss:0.5958\n","Training Epoch:20, Batch:95, Loss:0.5953\n","Training Epoch:20, Batch:100, Loss:0.5955\n","Training Epoch:20, Batch:105, Loss:0.5973\n","Training Epoch:20, Batch:110, Loss:0.5970\n","Training Epoch:20, Batch:115, Loss:0.5986\n","Training Epoch:20, Batch:120, Loss:0.5975\n","Training Epoch:20, Batch:125, Loss:0.5977\n","Training Epoch:20, Batch:130, Loss:0.5969\n","Training Epoch:20, Batch:135, Loss:0.5978\n","Training Epoch:20, Batch:140, Loss:0.5970\n","Training Epoch:20, Batch:145, Loss:0.5967\n","Training Epoch:20, Batch:150, Loss:0.5971\n","Training Epoch:20, Batch:155, Loss:0.5996\n","Training Epoch:20, Batch:160, Loss:0.5983\n","Training Epoch:20, Batch:165, Loss:0.5991\n","Training Epoch:20, Batch:170, Loss:0.5985\n","Training Epoch:20, Batch:175, Loss:0.5982\n","Training Epoch:20, Average Loss:0.5980\n","Validation Epoch:20, Average Loss:0.7224, Accuracy:0.7588\n","Training Epoch:21, Batch:5, Loss:0.5973\n","Training Epoch:21, Batch:10, Loss:0.5739\n","Training Epoch:21, Batch:15, Loss:0.5732\n","Training Epoch:21, Batch:20, Loss:0.5853\n","Training Epoch:21, Batch:25, Loss:0.5905\n","Training Epoch:21, Batch:30, Loss:0.5866\n","Training Epoch:21, Batch:35, Loss:0.5784\n","Training Epoch:21, Batch:40, Loss:0.5844\n","Training Epoch:21, Batch:45, Loss:0.5836\n","Training Epoch:21, Batch:50, Loss:0.5836\n","Training Epoch:21, Batch:55, Loss:0.5818\n","Training Epoch:21, Batch:60, Loss:0.5823\n","Training Epoch:21, Batch:65, Loss:0.5793\n","Training Epoch:21, Batch:70, Loss:0.5812\n","Training Epoch:21, Batch:75, Loss:0.5810\n","Training Epoch:21, Batch:80, Loss:0.5786\n","Training Epoch:21, Batch:85, Loss:0.5811\n","Training Epoch:21, Batch:90, Loss:0.5821\n","Training Epoch:21, Batch:95, Loss:0.5815\n","Training Epoch:21, Batch:100, Loss:0.5829\n","Training Epoch:21, Batch:105, Loss:0.5834\n","Training Epoch:21, Batch:110, Loss:0.5839\n","Training Epoch:21, Batch:115, Loss:0.5839\n","Training Epoch:21, Batch:120, Loss:0.5840\n","Training Epoch:21, Batch:125, Loss:0.5842\n","Training Epoch:21, Batch:130, Loss:0.5848\n","Training Epoch:21, Batch:135, Loss:0.5849\n","Training Epoch:21, Batch:140, Loss:0.5848\n","Training Epoch:21, Batch:145, Loss:0.5861\n","Training Epoch:21, Batch:150, Loss:0.5874\n","Training Epoch:21, Batch:155, Loss:0.5884\n","Training Epoch:21, Batch:160, Loss:0.5892\n","Training Epoch:21, Batch:165, Loss:0.5892\n","Training Epoch:21, Batch:170, Loss:0.5902\n","Training Epoch:21, Batch:175, Loss:0.5898\n","Training Epoch:21, Average Loss:0.5896\n","Validation Epoch:21, Average Loss:0.7631, Accuracy:0.7508\n","Training Epoch:22, Batch:5, Loss:0.6053\n","Training Epoch:22, Batch:10, Loss:0.6098\n","Training Epoch:22, Batch:15, Loss:0.5956\n","Training Epoch:22, Batch:20, Loss:0.5943\n","Training Epoch:22, Batch:25, Loss:0.5915\n","Training Epoch:22, Batch:30, Loss:0.5928\n","Training Epoch:22, Batch:35, Loss:0.5931\n","Training Epoch:22, Batch:40, Loss:0.5919\n","Training Epoch:22, Batch:45, Loss:0.5884\n","Training Epoch:22, Batch:50, Loss:0.5948\n","Training Epoch:22, Batch:55, Loss:0.5966\n","Training Epoch:22, Batch:60, Loss:0.5956\n","Training Epoch:22, Batch:65, Loss:0.5940\n","Training Epoch:22, Batch:70, Loss:0.5971\n","Training Epoch:22, Batch:75, Loss:0.5949\n","Training Epoch:22, Batch:80, Loss:0.5944\n","Training Epoch:22, Batch:85, Loss:0.5940\n","Training Epoch:22, Batch:90, Loss:0.5925\n","Training Epoch:22, Batch:95, Loss:0.5904\n","Training Epoch:22, Batch:100, Loss:0.5897\n","Training Epoch:22, Batch:105, Loss:0.5897\n","Training Epoch:22, Batch:110, Loss:0.5899\n","Training Epoch:22, Batch:115, Loss:0.5892\n","Training Epoch:22, Batch:120, Loss:0.5879\n","Training Epoch:22, Batch:125, Loss:0.5862\n","Training Epoch:22, Batch:130, Loss:0.5848\n","Training Epoch:22, Batch:135, Loss:0.5859\n","Training Epoch:22, Batch:140, Loss:0.5851\n","Training Epoch:22, Batch:145, Loss:0.5856\n","Training Epoch:22, Batch:150, Loss:0.5874\n","Training Epoch:22, Batch:155, Loss:0.5854\n","Training Epoch:22, Batch:160, Loss:0.5839\n","Training Epoch:22, Batch:165, Loss:0.5844\n","Training Epoch:22, Batch:170, Loss:0.5845\n","Training Epoch:22, Batch:175, Loss:0.5838\n","Training Epoch:22, Average Loss:0.5845\n","Validation Epoch:22, Average Loss:0.8936, Accuracy:0.7564\n","Training Epoch:23, Batch:5, Loss:0.6064\n","Training Epoch:23, Batch:10, Loss:0.6035\n","Training Epoch:23, Batch:15, Loss:0.5983\n","Training Epoch:23, Batch:20, Loss:0.5940\n","Training Epoch:23, Batch:25, Loss:0.5946\n","Training Epoch:23, Batch:30, Loss:0.5941\n","Training Epoch:23, Batch:35, Loss:0.5924\n","Training Epoch:23, Batch:40, Loss:0.5920\n","Training Epoch:23, Batch:45, Loss:0.5943\n","Training Epoch:23, Batch:50, Loss:0.5904\n","Training Epoch:23, Batch:55, Loss:0.5876\n","Training Epoch:23, Batch:60, Loss:0.5881\n","Training Epoch:23, Batch:65, Loss:0.5893\n","Training Epoch:23, Batch:70, Loss:0.5892\n","Training Epoch:23, Batch:75, Loss:0.5892\n","Training Epoch:23, Batch:80, Loss:0.5887\n","Training Epoch:23, Batch:85, Loss:0.5902\n","Training Epoch:23, Batch:90, Loss:0.5906\n","Training Epoch:23, Batch:95, Loss:0.5899\n","Training Epoch:23, Batch:100, Loss:0.5882\n","Training Epoch:23, Batch:105, Loss:0.5881\n","Training Epoch:23, Batch:110, Loss:0.5874\n","Training Epoch:23, Batch:115, Loss:0.5873\n","Training Epoch:23, Batch:120, Loss:0.5872\n","Training Epoch:23, Batch:125, Loss:0.5851\n","Training Epoch:23, Batch:130, Loss:0.5848\n","Training Epoch:23, Batch:135, Loss:0.5851\n","Training Epoch:23, Batch:140, Loss:0.5850\n","Training Epoch:23, Batch:145, Loss:0.5861\n","Training Epoch:23, Batch:150, Loss:0.5854\n","Training Epoch:23, Batch:155, Loss:0.5851\n","Training Epoch:23, Batch:160, Loss:0.5864\n","Training Epoch:23, Batch:165, Loss:0.5860\n","Training Epoch:23, Batch:170, Loss:0.5862\n","Training Epoch:23, Batch:175, Loss:0.5864\n","Training Epoch:23, Average Loss:0.5863\n","Validation Epoch:23, Average Loss:0.7229, Accuracy:0.7604\n","Training Epoch:24, Batch:5, Loss:0.5488\n","Training Epoch:24, Batch:10, Loss:0.5502\n","Training Epoch:24, Batch:15, Loss:0.5536\n","Training Epoch:24, Batch:20, Loss:0.5551\n","Training Epoch:24, Batch:25, Loss:0.5511\n","Training Epoch:24, Batch:30, Loss:0.5523\n","Training Epoch:24, Batch:35, Loss:0.5555\n","Training Epoch:24, Batch:40, Loss:0.5572\n","Training Epoch:24, Batch:45, Loss:0.5530\n","Training Epoch:24, Batch:50, Loss:0.5558\n","Training Epoch:24, Batch:55, Loss:0.5595\n","Training Epoch:24, Batch:60, Loss:0.5609\n","Training Epoch:24, Batch:65, Loss:0.5613\n","Training Epoch:24, Batch:70, Loss:0.5650\n","Training Epoch:24, Batch:75, Loss:0.5644\n","Training Epoch:24, Batch:80, Loss:0.5634\n","Training Epoch:24, Batch:85, Loss:0.5646\n","Training Epoch:24, Batch:90, Loss:0.5678\n","Training Epoch:24, Batch:95, Loss:0.5682\n","Training Epoch:24, Batch:100, Loss:0.5679\n","Training Epoch:24, Batch:105, Loss:0.5672\n","Training Epoch:24, Batch:110, Loss:0.5686\n","Training Epoch:24, Batch:115, Loss:0.5700\n","Training Epoch:24, Batch:120, Loss:0.5704\n","Training Epoch:24, Batch:125, Loss:0.5725\n","Training Epoch:24, Batch:130, Loss:0.5733\n","Training Epoch:24, Batch:135, Loss:0.5742\n","Training Epoch:24, Batch:140, Loss:0.5741\n","Training Epoch:24, Batch:145, Loss:0.5736\n","Training Epoch:24, Batch:150, Loss:0.5732\n","Training Epoch:24, Batch:155, Loss:0.5745\n","Training Epoch:24, Batch:160, Loss:0.5744\n","Training Epoch:24, Batch:165, Loss:0.5758\n","Training Epoch:24, Batch:170, Loss:0.5766\n","Training Epoch:24, Batch:175, Loss:0.5775\n","Training Epoch:24, Average Loss:0.5781\n","Validation Epoch:24, Average Loss:0.7966, Accuracy:0.7504\n","Training Epoch:25, Batch:5, Loss:0.5625\n","Training Epoch:25, Batch:10, Loss:0.5642\n","Training Epoch:25, Batch:15, Loss:0.5782\n","Training Epoch:25, Batch:20, Loss:0.5845\n","Training Epoch:25, Batch:25, Loss:0.5868\n","Training Epoch:25, Batch:30, Loss:0.5872\n","Training Epoch:25, Batch:35, Loss:0.5858\n","Training Epoch:25, Batch:40, Loss:0.5776\n","Training Epoch:25, Batch:45, Loss:0.5697\n","Training Epoch:25, Batch:50, Loss:0.5688\n","Training Epoch:25, Batch:55, Loss:0.5667\n","Training Epoch:25, Batch:60, Loss:0.5670\n","Training Epoch:25, Batch:65, Loss:0.5678\n","Training Epoch:25, Batch:70, Loss:0.5698\n","Training Epoch:25, Batch:75, Loss:0.5696\n","Training Epoch:25, Batch:80, Loss:0.5707\n","Training Epoch:25, Batch:85, Loss:0.5742\n","Training Epoch:25, Batch:90, Loss:0.5744\n","Training Epoch:25, Batch:95, Loss:0.5750\n","Training Epoch:25, Batch:100, Loss:0.5750\n","Training Epoch:25, Batch:105, Loss:0.5751\n","Training Epoch:25, Batch:110, Loss:0.5742\n","Training Epoch:25, Batch:115, Loss:0.5724\n","Training Epoch:25, Batch:120, Loss:0.5715\n","Training Epoch:25, Batch:125, Loss:0.5742\n","Training Epoch:25, Batch:130, Loss:0.5739\n","Training Epoch:25, Batch:135, Loss:0.5737\n","Training Epoch:25, Batch:140, Loss:0.5736\n","Training Epoch:25, Batch:145, Loss:0.5721\n","Training Epoch:25, Batch:150, Loss:0.5719\n","Training Epoch:25, Batch:155, Loss:0.5723\n","Training Epoch:25, Batch:160, Loss:0.5730\n","Training Epoch:25, Batch:165, Loss:0.5727\n","Training Epoch:25, Batch:170, Loss:0.5714\n","Training Epoch:25, Batch:175, Loss:0.5729\n","Training Epoch:25, Average Loss:0.5737\n","Validation Epoch:25, Average Loss:0.9169, Accuracy:0.7394\n","Training Epoch:26, Batch:5, Loss:0.5959\n","Training Epoch:26, Batch:10, Loss:0.5766\n","Training Epoch:26, Batch:15, Loss:0.5895\n","Training Epoch:26, Batch:20, Loss:0.5890\n","Training Epoch:26, Batch:25, Loss:0.5842\n","Training Epoch:26, Batch:30, Loss:0.5790\n","Training Epoch:26, Batch:35, Loss:0.5749\n","Training Epoch:26, Batch:40, Loss:0.5749\n","Training Epoch:26, Batch:45, Loss:0.5757\n","Training Epoch:26, Batch:50, Loss:0.5711\n","Training Epoch:26, Batch:55, Loss:0.5697\n","Training Epoch:26, Batch:60, Loss:0.5720\n","Training Epoch:26, Batch:65, Loss:0.5698\n","Training Epoch:26, Batch:70, Loss:0.5713\n","Training Epoch:26, Batch:75, Loss:0.5727\n","Training Epoch:26, Batch:80, Loss:0.5729\n","Training Epoch:26, Batch:85, Loss:0.5730\n","Training Epoch:26, Batch:90, Loss:0.5740\n","Training Epoch:26, Batch:95, Loss:0.5733\n","Training Epoch:26, Batch:100, Loss:0.5742\n","Training Epoch:26, Batch:105, Loss:0.5735\n","Training Epoch:26, Batch:110, Loss:0.5757\n","Training Epoch:26, Batch:115, Loss:0.5764\n","Training Epoch:26, Batch:120, Loss:0.5776\n","Training Epoch:26, Batch:125, Loss:0.5787\n","Training Epoch:26, Batch:130, Loss:0.5776\n","Training Epoch:26, Batch:135, Loss:0.5770\n","Training Epoch:26, Batch:140, Loss:0.5770\n","Training Epoch:26, Batch:145, Loss:0.5762\n","Training Epoch:26, Batch:150, Loss:0.5768\n","Training Epoch:26, Batch:155, Loss:0.5756\n","Training Epoch:26, Batch:160, Loss:0.5753\n","Training Epoch:26, Batch:165, Loss:0.5743\n","Training Epoch:26, Batch:170, Loss:0.5743\n","Training Epoch:26, Batch:175, Loss:0.5754\n","Training Epoch:26, Average Loss:0.5755\n","Validation Epoch:26, Average Loss:0.8473, Accuracy:0.7492\n","Training Epoch:27, Batch:5, Loss:0.5285\n","Training Epoch:27, Batch:10, Loss:0.5527\n","Training Epoch:27, Batch:15, Loss:0.5600\n","Training Epoch:27, Batch:20, Loss:0.5575\n","Training Epoch:27, Batch:25, Loss:0.5518\n","Training Epoch:27, Batch:30, Loss:0.5515\n","Training Epoch:27, Batch:35, Loss:0.5546\n","Training Epoch:27, Batch:40, Loss:0.5568\n","Training Epoch:27, Batch:45, Loss:0.5565\n","Training Epoch:27, Batch:50, Loss:0.5621\n","Training Epoch:27, Batch:55, Loss:0.5639\n","Training Epoch:27, Batch:60, Loss:0.5631\n","Training Epoch:27, Batch:65, Loss:0.5675\n","Training Epoch:27, Batch:70, Loss:0.5668\n","Training Epoch:27, Batch:75, Loss:0.5677\n","Training Epoch:27, Batch:80, Loss:0.5639\n","Training Epoch:27, Batch:85, Loss:0.5621\n","Training Epoch:27, Batch:90, Loss:0.5629\n","Training Epoch:27, Batch:95, Loss:0.5639\n","Training Epoch:27, Batch:100, Loss:0.5650\n","Training Epoch:27, Batch:105, Loss:0.5662\n","Training Epoch:27, Batch:110, Loss:0.5655\n","Training Epoch:27, Batch:115, Loss:0.5665\n","Training Epoch:27, Batch:120, Loss:0.5654\n","Training Epoch:27, Batch:125, Loss:0.5638\n","Training Epoch:27, Batch:130, Loss:0.5644\n","Training Epoch:27, Batch:135, Loss:0.5647\n","Training Epoch:27, Batch:140, Loss:0.5667\n","Training Epoch:27, Batch:145, Loss:0.5663\n","Training Epoch:27, Batch:150, Loss:0.5681\n","Training Epoch:27, Batch:155, Loss:0.5686\n","Training Epoch:27, Batch:160, Loss:0.5685\n","Training Epoch:27, Batch:165, Loss:0.5676\n","Training Epoch:27, Batch:170, Loss:0.5666\n","Training Epoch:27, Batch:175, Loss:0.5675\n","Training Epoch:27, Average Loss:0.5680\n","Validation Epoch:27, Average Loss:0.7564, Accuracy:0.7534\n","Training Epoch:28, Batch:5, Loss:0.5600\n","Training Epoch:28, Batch:10, Loss:0.5675\n","Training Epoch:28, Batch:15, Loss:0.5601\n","Training Epoch:28, Batch:20, Loss:0.5688\n","Training Epoch:28, Batch:25, Loss:0.5637\n","Training Epoch:28, Batch:30, Loss:0.5584\n","Training Epoch:28, Batch:35, Loss:0.5563\n","Training Epoch:28, Batch:40, Loss:0.5546\n","Training Epoch:28, Batch:45, Loss:0.5583\n","Training Epoch:28, Batch:50, Loss:0.5579\n","Training Epoch:28, Batch:55, Loss:0.5610\n","Training Epoch:28, Batch:60, Loss:0.5605\n","Training Epoch:28, Batch:65, Loss:0.5596\n","Training Epoch:28, Batch:70, Loss:0.5613\n","Training Epoch:28, Batch:75, Loss:0.5610\n","Training Epoch:28, Batch:80, Loss:0.5591\n","Training Epoch:28, Batch:85, Loss:0.5607\n","Training Epoch:28, Batch:90, Loss:0.5633\n","Training Epoch:28, Batch:95, Loss:0.5639\n","Training Epoch:28, Batch:100, Loss:0.5631\n","Training Epoch:28, Batch:105, Loss:0.5647\n","Training Epoch:28, Batch:110, Loss:0.5644\n","Training Epoch:28, Batch:115, Loss:0.5645\n","Training Epoch:28, Batch:120, Loss:0.5643\n","Training Epoch:28, Batch:125, Loss:0.5653\n","Training Epoch:28, Batch:130, Loss:0.5647\n","Training Epoch:28, Batch:135, Loss:0.5651\n","Training Epoch:28, Batch:140, Loss:0.5661\n","Training Epoch:28, Batch:145, Loss:0.5666\n","Training Epoch:28, Batch:150, Loss:0.5685\n","Training Epoch:28, Batch:155, Loss:0.5678\n","Training Epoch:28, Batch:160, Loss:0.5687\n","Training Epoch:28, Batch:165, Loss:0.5694\n","Training Epoch:28, Batch:170, Loss:0.5699\n","Training Epoch:28, Batch:175, Loss:0.5700\n","Training Epoch:28, Average Loss:0.5699\n","Validation Epoch:28, Average Loss:0.7300, Accuracy:0.7488\n","Training Epoch:29, Batch:5, Loss:0.5356\n","Training Epoch:29, Batch:10, Loss:0.5486\n","Training Epoch:29, Batch:15, Loss:0.5506\n","Training Epoch:29, Batch:20, Loss:0.5400\n","Training Epoch:29, Batch:25, Loss:0.5336\n","Training Epoch:29, Batch:30, Loss:0.5416\n","Training Epoch:29, Batch:35, Loss:0.5471\n","Training Epoch:29, Batch:40, Loss:0.5494\n","Training Epoch:29, Batch:45, Loss:0.5490\n","Training Epoch:29, Batch:50, Loss:0.5470\n","Training Epoch:29, Batch:55, Loss:0.5476\n","Training Epoch:29, Batch:60, Loss:0.5465\n","Training Epoch:29, Batch:65, Loss:0.5495\n","Training Epoch:29, Batch:70, Loss:0.5506\n","Training Epoch:29, Batch:75, Loss:0.5492\n","Training Epoch:29, Batch:80, Loss:0.5489\n","Training Epoch:29, Batch:85, Loss:0.5473\n","Training Epoch:29, Batch:90, Loss:0.5485\n","Training Epoch:29, Batch:95, Loss:0.5525\n","Training Epoch:29, Batch:100, Loss:0.5559\n","Training Epoch:29, Batch:105, Loss:0.5570\n","Training Epoch:29, Batch:110, Loss:0.5572\n","Training Epoch:29, Batch:115, Loss:0.5569\n","Training Epoch:29, Batch:120, Loss:0.5588\n","Training Epoch:29, Batch:125, Loss:0.5598\n","Training Epoch:29, Batch:130, Loss:0.5598\n","Training Epoch:29, Batch:135, Loss:0.5593\n","Training Epoch:29, Batch:140, Loss:0.5599\n","Training Epoch:29, Batch:145, Loss:0.5597\n","Training Epoch:29, Batch:150, Loss:0.5619\n","Training Epoch:29, Batch:155, Loss:0.5619\n","Training Epoch:29, Batch:160, Loss:0.5617\n","Training Epoch:29, Batch:165, Loss:0.5620\n","Training Epoch:29, Batch:170, Loss:0.5617\n","Training Epoch:29, Batch:175, Loss:0.5603\n","Training Epoch:29, Average Loss:0.5597\n","Validation Epoch:29, Average Loss:0.7091, Accuracy:0.7492\n","Training Epoch:30, Batch:5, Loss:0.5507\n","Training Epoch:30, Batch:10, Loss:0.5544\n","Training Epoch:30, Batch:15, Loss:0.5338\n","Training Epoch:30, Batch:20, Loss:0.5375\n","Training Epoch:30, Batch:25, Loss:0.5270\n","Training Epoch:30, Batch:30, Loss:0.5249\n","Training Epoch:30, Batch:35, Loss:0.5292\n","Training Epoch:30, Batch:40, Loss:0.5315\n","Training Epoch:30, Batch:45, Loss:0.5330\n","Training Epoch:30, Batch:50, Loss:0.5379\n","Training Epoch:30, Batch:55, Loss:0.5396\n","Training Epoch:30, Batch:60, Loss:0.5409\n","Training Epoch:30, Batch:65, Loss:0.5442\n","Training Epoch:30, Batch:70, Loss:0.5449\n","Training Epoch:30, Batch:75, Loss:0.5461\n","Training Epoch:30, Batch:80, Loss:0.5479\n","Training Epoch:30, Batch:85, Loss:0.5500\n","Training Epoch:30, Batch:90, Loss:0.5495\n","Training Epoch:30, Batch:95, Loss:0.5514\n","Training Epoch:30, Batch:100, Loss:0.5526\n","Training Epoch:30, Batch:105, Loss:0.5524\n","Training Epoch:30, Batch:110, Loss:0.5528\n","Training Epoch:30, Batch:115, Loss:0.5518\n","Training Epoch:30, Batch:120, Loss:0.5516\n","Training Epoch:30, Batch:125, Loss:0.5525\n","Training Epoch:30, Batch:130, Loss:0.5534\n","Training Epoch:30, Batch:135, Loss:0.5536\n","Training Epoch:30, Batch:140, Loss:0.5535\n","Training Epoch:30, Batch:145, Loss:0.5541\n","Training Epoch:30, Batch:150, Loss:0.5540\n","Training Epoch:30, Batch:155, Loss:0.5540\n","Training Epoch:30, Batch:160, Loss:0.5545\n","Training Epoch:30, Batch:165, Loss:0.5561\n","Training Epoch:30, Batch:170, Loss:0.5564\n","Training Epoch:30, Batch:175, Loss:0.5570\n","Training Epoch:30, Average Loss:0.5569\n","Validation Epoch:30, Average Loss:0.7566, Accuracy:0.7548\n","Training Epoch:31, Batch:5, Loss:0.5650\n","Training Epoch:31, Batch:10, Loss:0.5511\n","Training Epoch:31, Batch:15, Loss:0.5537\n","Training Epoch:31, Batch:20, Loss:0.5553\n","Training Epoch:31, Batch:25, Loss:0.5543\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-43b27180495f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROGRESS_INTERVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-80-5dd508c4bbaa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, progress_interval, patience)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 156 batches # 175 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0mcenter_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[1;32m   2154\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAFFINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfillcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[1;32m   2495\u001b[0m                 \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQUAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m             im.__transformer(\n\u001b[0m\u001b[1;32m   2498\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__transformer\u001b[0;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["model_name= 'model50second_ep30_add_drop+aug'\n","model_path = f'drive/MyDrive/mlcs/resnet/{model_name}.pth'\n","torch.save(model, model_path)"],"metadata":{"id":"m5NSvQLCCT8g","executionInfo":{"status":"ok","timestamp":1689862009521,"user_tz":-540,"elapsed":674,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["import csv\n","\n","with open(f'./loss_{model_name}.csv','w') as file:\n","  writer = csv.writer(file)\n","  writer.writerow(['train_loss', 'val_loss'])\n","  for train_loss, val_loss in zip(train_loss_list, val_loss_list):\n","    writer.writerow([train_loss, val_loss])"],"metadata":{"id":"hKDT_9ILwlVb","executionInfo":{"status":"ok","timestamp":1689862012202,"user_tz":-540,"elapsed":1,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["def test(model):\n","    model.eval()\n","    with torch.no_grad():\n","        test_loss = 0\n","        correct_cnt=0\n","        total_cnt= 0\n","\n","        for batch_i, (data, target) in enumerate(test_loader):\n","            if torch.cuda.is_available() is True:\n","                data, target = data.to(device), target.to(device)\n","            total_cnt += target.size(0)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            _, pred_label = torch.max(nn.functional.softmax(output,dim=1),1)\n","            correct_cnt += (pred_label==target).sum().item()\n","            test_loss += loss.item()\n","\n","    total_loss = test_loss/len(test_loader)\n","    acu = correct_cnt/total_cnt\n","    print(f\"Test Loss:{total_loss}, Accuracy:{acu:.4f}\")\n"],"metadata":{"id":"hMdwtToiFAfk","executionInfo":{"status":"ok","timestamp":1689862015796,"user_tz":-540,"elapsed":3,"user":{"displayName":"최석근","userId":"12650075805346531848"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["test(model)"],"metadata":{"id":"BUqLvk8fFAmG","executionInfo":{"status":"ok","timestamp":1689862018350,"user_tz":-540,"elapsed":2556,"user":{"displayName":"최석근","userId":"12650075805346531848"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e118979-c37e-496d-aaef-60f0c2e85c05"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss:0.7233062237501144, Accuracy:0.7913\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VspxV4k0_7-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eY8ck48eEzyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dCviNaQQAxsU"},"execution_count":null,"outputs":[]}]}